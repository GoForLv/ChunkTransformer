********************************************************************************
使用MyTransformer，传统多头注意力作为对照，每批次至少15s。
2025-03-26 02:24:24
model: MyTransformer, min_loss: 2.1679, Train Time: 12m54.42s, Attention Time: 0m14.28s, Attention Count: 80700
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4
nhead: 8, num_encoder_layers: 6
seq_len: 256, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
使用MyTransformer，传统多头注意力作为对照，每批次至少15s。
2025-03-26 02:43:16
model: MyTransformer, min_loss: 2.0012, Train Time: 12m51.15s, Attention Time: 0m13.07s, Attention Count: 80700
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4
nhead: 8, num_encoder_layers: 6
seq_len: 256, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
使用MyTransformer，传统多头注意力作为对照，每批次至少15s。
2025-03-26 02:59:11
model: MyTransformer, min_loss: 2.1811, Train Time: 12m51.32s, Attention Time: 0m12.90s, Attention Count: 80700
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4
nhead: 8, num_encoder_layers: 6
seq_len: 256, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
使用MyTransformer，传统多头注意力作为对照，每批次至少15s。
2025-03-26 03:40:01
model: MyTransformer, min_loss: 2.1851, Train Time: 12m51.64s, Attention Time: 0m12.86s, Attention Count: 80700
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4
nhead: 8, num_encoder_layers: 6
seq_len: 256, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
使用分块多头注意力，实验组，每批次至少7s。
2025-03-26 03:08:07
model: MyTransformer, min_loss: 2.2421, Train Time: 6m15.38s, Attention Time: 0m14.03s, Attention Count: 80700
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4
nhead: 8, num_encoder_layers: 6
seq_len: 256, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
使用分块多头注意力，实验组，每批次至少7s。
2025-03-26 03:15:48
model: MyTransformer, min_loss: 2.2685, Train Time: 6m15.83s, Attention Time: 0m14.20s, Attention Count: 80700
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4
nhead: 8, num_encoder_layers: 6
seq_len: 256, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
使用分块多头注意力，实验组，每批次至少7s。
2025-03-26 03:25:31
model: MyTransformer, min_loss: 2.3863, Train Time: 6m15.41s, Attention Time: 0m14.05s, Attention Count: 80700
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4
nhead: 8, num_encoder_layers: 6
seq_len: 256, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

#################################################################################

********************************************************************************
使用MyTransformer，传统多头注意力作为对照，每批次至少51s。
2025-03-26 04:27:55
model: MyTransformer, min_loss: 2.0822, Train Time: 42m43.50s, Attention Time: 11936.790
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4
nhead: 8, num_encoder_layers: 6
seq_len: 512, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
使用MyTransformer，传统多头注意力作为对照，每批次至少51s。
2025-03-26 05:11:02
model: MyTransformer, min_loss: 2.1738, Train Time: 42m43.68s, Attention Time: 11819.409
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4
nhead: 8, num_encoder_layers: 6
seq_len: 512, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
使用MyTransformer，传统多头注意力作为对照，每批次至少51s。
2025-03-26 05:54:20
model: MyTransformer, min_loss: 2.0894, Train Time: 42m53.4s, Attention Time: 12148.416
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4
nhead: 8, num_encoder_layers: 6
seq_len: 512, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
使用MyTransformer，传统多头注意力作为对照，每批次至少14s。
2025-03-26 06:06:56
model: MyTransformer, min_loss: 2.4115, Train Time: 12m4.36s, Attention Time: 13323.941
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4
nhead: 8, num_encoder_layers: 6
seq_len: 512, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
使用MyTransformer，传统多头注意力作为对照，每批次至少14s。
2025-03-26 06:19:18
model: MyTransformer, min_loss: 2.3439, Train Time: 24m8.79s, Attention Time: 26644.610
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4
nhead: 8, num_encoder_layers: 6
seq_len: 512, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
使用MyTransformer，传统多头注意力作为对照，每批次至少14s。
2025-03-26 06:31:45
model: MyTransformer, min_loss: 2.2116, Train Time: 36m16.99s, Attention Time: 39938.544
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4
nhead: 8, num_encoder_layers: 6
seq_len: 512, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************

################################################################################
优化测试和辅助工具
################################################################################
