********************************************************************************
使用MyTransformer，传统多头注意力作为对照，每批次至少15s。
2025-03-26 02:24:24
model: MyTransformer, min_loss: 2.1679, Train Time: 12m54.42s, Attention Time: 0m14.28s, Attention Count: 80700
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4
nhead: 8, num_encoder_layers: 6
seq_len: 256, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
使用MyTransformer，传统多头注意力作为对照，每批次至少15s。
2025-03-26 02:43:16
model: MyTransformer, min_loss: 2.0012, Train Time: 12m51.15s, Attention Time: 0m13.07s, Attention Count: 80700
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4
nhead: 8, num_encoder_layers: 6
seq_len: 256, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
使用MyTransformer，传统多头注意力作为对照，每批次至少15s。
2025-03-26 02:59:11
model: MyTransformer, min_loss: 2.1811, Train Time: 12m51.32s, Attention Time: 0m12.90s, Attention Count: 80700
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4
nhead: 8, num_encoder_layers: 6
seq_len: 256, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
使用MyTransformer，传统多头注意力作为对照，每批次至少15s。
2025-03-26 03:40:01
model: MyTransformer, min_loss: 2.1851, Train Time: 12m51.64s, Attention Time: 0m12.86s, Attention Count: 80700
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4
nhead: 8, num_encoder_layers: 6
seq_len: 256, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
使用分块多头注意力，实验组，每批次至少7s。
2025-03-26 03:08:07
model: MyTransformer, min_loss: 2.2421, Train Time: 6m15.38s, Attention Time: 0m14.03s, Attention Count: 80700
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4
nhead: 8, num_encoder_layers: 6
seq_len: 256, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
使用分块多头注意力，实验组，每批次至少7s。
2025-03-26 03:15:48
model: MyTransformer, min_loss: 2.2685, Train Time: 6m15.83s, Attention Time: 0m14.20s, Attention Count: 80700
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4
nhead: 8, num_encoder_layers: 6
seq_len: 256, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
使用分块多头注意力，实验组，每批次至少7s。
2025-03-26 03:25:31
model: MyTransformer, min_loss: 2.3863, Train Time: 6m15.41s, Attention Time: 0m14.05s, Attention Count: 80700
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4
nhead: 8, num_encoder_layers: 6
seq_len: 256, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

#################################################################################

********************************************************************************
使用MyTransformer，传统多头注意力作为对照，每批次至少51s。
2025-03-26 04:27:55
model: MyTransformer, min_loss: 2.0822, Train Time: 42m43.50s, Attention Time: 11936.790
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4
nhead: 8, num_encoder_layers: 6
seq_len: 512, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
使用MyTransformer，传统多头注意力作为对照，每批次至少51s。
2025-03-26 05:11:02
model: MyTransformer, min_loss: 2.1738, Train Time: 42m43.68s, Attention Time: 11819.409
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4
nhead: 8, num_encoder_layers: 6
seq_len: 512, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
使用MyTransformer，传统多头注意力作为对照，每批次至少51s。
2025-03-26 05:54:20
model: MyTransformer, min_loss: 2.0894, Train Time: 42m53.4s, Attention Time: 12148.416
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4
nhead: 8, num_encoder_layers: 6
seq_len: 512, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
使用MyTransformer，传统多头注意力作为对照，每批次至少14s。
2025-03-26 06:06:56
model: MyTransformer, min_loss: 2.4115, Train Time: 12m4.36s, Attention Time: 13323.941
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4
nhead: 8, num_encoder_layers: 6
seq_len: 512, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
使用MyTransformer，传统多头注意力作为对照，每批次至少14s。
2025-03-26 06:19:18
model: MyTransformer, min_loss: 2.3439, Train Time: 12m4.43s, Attention Time: 26644.610
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4
nhead: 8, num_encoder_layers: 6
seq_len: 512, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
使用MyTransformer，传统多头注意力作为对照，每批次至少14s。
2025-03-26 06:31:45
model: MyTransformer, min_loss: 2.2116, Train Time: 12m8.20s, Attention Time: 39938.544
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4
nhead: 8, num_encoder_layers: 6
seq_len: 512, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

################################################################################
优化测试和辅助工具
################################################################################

********************************************************************************
log:
2025-03-26 14:51:02
model: OriginalTransformer, min_loss: 1.9415, peak_memory: 2344.827MB
--------------------------------------------------------------------------------
Phase     train     forward   criterion backward  optimizer test      
Time /s   15.788    5.869     0.052     9.258     0.389     0.050     
--------------------------------------------------------------------------------
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4, d_chunk: 8
nhead: 8, num_encoder_layers: 6
seq_len: 256, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
log:
2025-03-26 15:04:24
model: OriginalTransformer, min_loss: 2.0634, peak_memory: 2344.827MB
--------------------------------------------------------------------------------
Phase     train     forward   criterion backward  optimizer test      
Time /s   15.815    5.875     0.052     9.272     0.393     0.050     
--------------------------------------------------------------------------------
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4, d_chunk: 8
nhead: 8, num_encoder_layers: 6
seq_len: 256, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
log:
2025-03-26 15:17:45
model: OriginalTransformer, min_loss: 1.9587, peak_memory: 2344.827MB
--------------------------------------------------------------------------------
Phase     train     forward   criterion backward  optimizer test      
Time /s   15.811    5.875     0.051     9.271     0.392     0.050     
--------------------------------------------------------------------------------
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4, d_chunk: 8
nhead: 8, num_encoder_layers: 6
seq_len: 256, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
log: 
2025-03-26 16:01:09
model: OriginalTransformer, min_loss: 2.0263, peak_memory: 8573.515MB
--------------------------------------------------------------------------------
Phase     train     forward   criterion backward  optimizer test      
Time /s   51.521    18.869    0.070     31.840    0.445     0.155     
--------------------------------------------------------------------------------
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4, d_chunk: 8
nhead: 8, num_encoder_layers: 6
seq_len: 512, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
log: 
2025-03-26 16:44:28
model: OriginalTransformer, min_loss: 2.1663, peak_memory: 8573.515MB
--------------------------------------------------------------------------------
Phase     train     forward   criterion backward  optimizer test      
Time /s   51.514    18.867    0.070     31.834    0.450     0.155     
--------------------------------------------------------------------------------
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4, d_chunk: 8
nhead: 8, num_encoder_layers: 6
seq_len: 512, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

################################################################################
性能发生变化，重新测试
################################################################################

********************************************************************************
log: 
2025-03-27 05:29:17
model: OriginalTransformer, min_loss: 2.0306, peak_memory: 2344.827MB
--------------------------------------------------------------------------------
Phase     train     forward   criterion backward  optimizer test      
Time /s   17.087    6.378     0.047     10.079    0.370     0.054     
--------------------------------------------------------------------------------
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4, d_chunk: 8
nhead: 8, num_encoder_layers: 6
seq_len: 256, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
log: 
2025-03-27 05:50:06
model: OriginalTransformer, min_loss: 2.0081, peak_memory: 2344.827MB
--------------------------------------------------------------------------------
Phase     train     forward   criterion backward  optimizer test      
Time /s   17.114    6.387     0.047     10.094    0.373     0.054     
--------------------------------------------------------------------------------
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4, d_chunk: 8
nhead: 8, num_encoder_layers: 6
seq_len: 256, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
log: 
2025-03-27 06:04:33
model: OriginalTransformer, min_loss: 2.1561, peak_memory: 2344.827MB
--------------------------------------------------------------------------------
Phase     train     forward   criterion backward  optimizer test      
Time /s   17.119    6.386     0.047     10.098    0.374     0.054     
--------------------------------------------------------------------------------
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4, d_chunk: 8
nhead: 8, num_encoder_layers: 6
seq_len: 256, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
log: 
2025-03-27 06:19:00
model: OriginalTransformer, min_loss: 1.9871, peak_memory: 2344.827MB
--------------------------------------------------------------------------------
Phase     train     forward   criterion backward  optimizer test      
Time /s   17.119    6.387     0.047     10.099    0.374     0.054     
--------------------------------------------------------------------------------
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4, d_chunk: 8
nhead: 8, num_encoder_layers: 6
seq_len: 256, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
log: 
2025-03-27 07:06:31
model: OriginalTransformer, min_loss: 2.2792, peak_memory: 8573.515MB
--------------------------------------------------------------------------------
Phase     train     forward   criterion backward  optimizer test      
Time /s   56.440    20.718    0.055     35.000    0.392     0.169     
--------------------------------------------------------------------------------
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4, d_chunk: 8
nhead: 8, num_encoder_layers: 6
seq_len: 512, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
log: 
2025-03-27 07:53:58
model: OriginalTransformer, min_loss: 2.2824, peak_memory: 8573.515MB
--------------------------------------------------------------------------------
Phase     train     forward   criterion backward  optimizer test      
Time /s   56.451    20.715    0.055     35.005    0.397     0.169     
--------------------------------------------------------------------------------
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4, d_chunk: 8
nhead: 8, num_encoder_layers: 6
seq_len: 512, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
log: 
2025-03-27 08:41:25
model: OriginalTransformer, min_loss: 2.3095, peak_memory: 8573.515MB
--------------------------------------------------------------------------------
Phase     train     forward   criterion backward  optimizer test      
Time /s   56.447    20.719    0.056     34.998    0.397     0.168     
--------------------------------------------------------------------------------
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4, d_chunk: 8
nhead: 8, num_encoder_layers: 6
seq_len: 512, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
log: 
2025-03-27 08:48:26
model: MyTransformer, min_loss: 2.2823, peak_memory: 667.744MB
--------------------------------------------------------------------------------
Phase     train     forward   criterion backward  optimizer test      
Time /s   8.148     2.406     0.044     5.147     0.366     0.024     
--------------------------------------------------------------------------------
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4, d_chunk: 8
nhead: 8, num_encoder_layers: 6
seq_len: 256, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
log: 
2025-03-27 08:55:25
model: MyTransformer, min_loss: 2.1790, peak_memory: 667.744MB
--------------------------------------------------------------------------------
Phase     train     forward   criterion backward  optimizer test      
Time /s   8.169     2.425     0.045     5.146     0.367     0.024     
--------------------------------------------------------------------------------
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4, d_chunk: 8
nhead: 8, num_encoder_layers: 6
seq_len: 256, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
log: 
2025-03-27 09:02:24
model: MyTransformer, min_loss: 2.1395, peak_memory: 667.744MB
--------------------------------------------------------------------------------
Phase     train     forward   criterion backward  optimizer test      
Time /s   8.172     2.427     0.045     5.146     0.368     0.024     
--------------------------------------------------------------------------------
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4, d_chunk: 8
nhead: 8, num_encoder_layers: 6
seq_len: 256, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
log: 
2025-03-27 09:15:38
model: MyTransformer, min_loss: 2.4069, peak_memory: 1315.494MB
--------------------------------------------------------------------------------
Phase     train     forward   criterion backward  optimizer test      
Time /s   15.429    4.776     0.045     10.024    0.354     0.046     
--------------------------------------------------------------------------------
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4, d_chunk: 8
nhead: 8, num_encoder_layers: 6
seq_len: 512, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
log: 
2025-03-27 09:28:46
model: MyTransformer, min_loss: 2.3519, peak_memory: 1315.494MB
--------------------------------------------------------------------------------
Phase     train     forward   criterion backward  optimizer test      
Time /s   15.417    4.775     0.046     10.012    0.354     0.046     
--------------------------------------------------------------------------------
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4, d_chunk: 8
nhead: 8, num_encoder_layers: 6
seq_len: 512, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
log: 
2025-03-27 09:41:57
model: MyTransformer, min_loss: 2.2786, peak_memory: 1315.494MB
--------------------------------------------------------------------------------
Phase     train     forward   criterion backward  optimizer test      
Time /s   15.415    4.766     0.046     10.017    0.356     0.046     
--------------------------------------------------------------------------------
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4, d_chunk: 8
nhead: 8, num_encoder_layers: 6
seq_len: 512, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
log: 
2025-03-27 09:55:06
model: MyTransformer, min_loss: 2.4039, peak_memory: 1315.494MB
--------------------------------------------------------------------------------
Phase     train     forward   criterion backward  optimizer test      
Time /s   15.452    4.810     0.046     10.006    0.356     0.046     
--------------------------------------------------------------------------------
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4, d_chunk: 8
nhead: 8, num_encoder_layers: 6
seq_len: 512, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
log: 
2025-03-27 10:08:19
model: MyTransformer, min_loss: 2.1837, peak_memory: 1315.494MB
--------------------------------------------------------------------------------
Phase     train     forward   criterion backward  optimizer test      
Time /s   15.457    4.823     0.046     10.003    0.356     0.046     
--------------------------------------------------------------------------------
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4, d_chunk: 8
nhead: 8, num_encoder_layers: 6
seq_len: 512, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

********************************************************************************
log: 
2025-03-27 10:21:31
model: MyTransformer, min_loss: 2.3931, peak_memory: 1315.494MB
--------------------------------------------------------------------------------
Phase     train     forward   criterion backward  optimizer test      
Time /s   15.480    4.857     0.046     9.990     0.355     0.047     
--------------------------------------------------------------------------------
d_model: 64, d_ffn: 256, d_input: 7, d_output: 7
n_neighbor: 4, d_chunk: 8
nhead: 8, num_encoder_layers: 6
seq_len: 512, epochs: 50, batch_size: 64, lr: 0.0005, dropout: 0.1

